[trainer]
batch_size = 100




[train]
rl_learning_rate = 0.001
# number of batches to train at the end of training episode
train_batches = 100
# training episodes in outer loop
train_episodes = 5
# number of episodes sampled in one training episode
sample_episodes = 1
target_update_interval = 50
evaluation_interval = 1000
# the memory pool can roughly store 2K episodes, total size = episodes * 50
capacity = 100000
epsilon_start = 0.5
epsilon_end = 0.1
epsilon_decay = 4000
checkpoint_interval = 1000
